{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning WORD2VEC: K-Means Clustering\n",
    "----\n",
    "Goal: Because the dataset contains variable-length reviews, we need to **transform individual word vectors into a feature set that is the same length for every review.**\n",
    "\n",
    "WORD2VEC creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as **vector quantization**. To accomplish vector quantization, we first need to find the centers of the word clusters, which we can do using a [clustering algorithm](http://scikit-learn.org/stable/modules/clustering.html) such as [K-Means Clustering Algorithm](http://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "## [K-Means Clustering](http://en.wikipedia.org/wiki/K-means_clustering)\n",
    "In K-Means, the one parameter we need to set is **K --the number of clusters**. \n",
    "\n",
    "How we decide how many clusters to create? Trial and error suggested that **small clusters, with an average of 5 words per cluster,** give better than large clusters with many words. \n",
    "\n",
    "We use [scikit-learn to perform our K-Means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code can take ~40 minutes. A time around the K-Means function shows how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load the model that we created in Part 2\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "print type(model.syn0)\n",
    "print model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a k-means object and use it to extract centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K-Means Clustering: 537 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "# Start time --to demonstrate how long K-Means takes\n",
    "start_time   = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size\n",
    "# or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx               = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# Get end time\n",
    "end_time     = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print \"Time taken for K-Means Clustering: %d seconds\" % (elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cluster asssignment** is stored in `idx`, and the vocabulary from our original Word2Vec model is still stored in `model.index2word`. \n",
    "\n",
    "For convenience, we zip the cluster assigment and vocabulary into on dictionary: \n",
    "\n",
    "| | |\n",
    "|----|----|\n",
    "| [**zip**](https://docs.python.org/3/library/functions.html#zip) | Makes an iterator that aggregates the items from each input-iterable *(built-in)* |\n",
    "| [**dict**](https://docs.python.org/3/library/stdtypes.html#typesmapping) | Makes a dictionary that maps hashtable values to arbitrary objects |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary --mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain.  Here is a loop that prints out the words flor cluters 0 through 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "\taltogether\n",
      "\tsetup\n",
      "\tpotentially\n",
      "\n",
      "Cluster 1\n",
      "\tcoin\n",
      "\tbeside\n",
      "\n",
      "Cluster 2\n",
      "\tcaron\n",
      "\tnielsen\n",
      "\tleslie\n",
      "\n",
      "Cluster 3\n",
      "\tdevout\n",
      "\toccult\n",
      "\tpervert\n",
      "\tbuddhist\n",
      "\n",
      "Cluster 4\n",
      "\tpurists\n",
      "\n",
      "Cluster 5\n",
      "\tchong\n",
      "\tcheech\n",
      "\tmarin\n",
      "\n",
      "Cluster 6\n",
      "\ttossed\n",
      "\tsped\n",
      "\tcreeps\n",
      "\tchalk\n",
      "\tmesses\n",
      "\tticks\n",
      "\tslapped\n",
      "\tpumped\n",
      "\n",
      "Cluster 7\n",
      "\tspeedy\n",
      "\tcanned\n",
      "\tdroning\n",
      "\ttrendy\n",
      "\tghastly\n",
      "\tdrenched\n",
      "\tkitsch\n",
      "\tsaturated\n",
      "\n",
      "Cluster 8\n",
      "\trevisit\n",
      "\tdistribute\n",
      "\texecutives\n",
      "\texecs\n",
      "\tcompete\n",
      "\tacquire\n",
      "\tdared\n",
      "\tpromotion\n",
      "\tfinancing\n",
      "\treleasing\n",
      "\tcompanies\n",
      "\tdistributors\n",
      "\tcollectors\n",
      "\taccess\n",
      "\tmarketing\n",
      "\tpromote\n",
      "\tpublicity\n",
      "\tprofit\n",
      "\n",
      "Cluster 9\n",
      "\tluzhin\n",
      "\tfranchot\n",
      "\tbefuddled\n",
      "\tconsummate\n",
      "\tdyan\n",
      "\tjacob\n",
      "\twilled\n",
      "\tloveable\n",
      "\tfoil\n",
      "\tscheider\n",
      "\tstoic\n",
      "\tamiable\n",
      "\tbravado\n",
      "\tdevilish\n",
      "\taffable\n",
      "\tmacgregor\n",
      "\tmcgregor\n",
      "\thilt\n",
      "\tshrewd\n",
      "\tchanning\n"
     ]
    }
   ],
   "source": [
    "# Note: Your clusters may differ, as `Word2Vec` relies on a random number seed.\n",
    "\n",
    "\n",
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    \n",
    "    # Print the cluster number\n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    \n",
    "    # Find all words for that cluster number, print them out\n",
    "    # words = []\n",
    "    for i in xrange(0, len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster):\n",
    "            print \"\\t\" + word_centroid_map.keys()[i]\n",
    "    \n",
    "    # print words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are of varying quality. Some make sense \n",
    "\n",
    "- Cluster ? mostly contain names\n",
    "- Cluster ? mostly contain related adjectives\n",
    "- Cluster ? however is mystifying. What do the words have in common?\n",
    "\n",
    "Perhaps our algorithm works best on adjectives\n",
    "\n",
    "At any rate, now we have a cluster (or *centroid*) assignment for each word. Now, we can define a function to convert reviews into **bags-of-centroids**. This works just like Bag-of-Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    #\n",
    "    \n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word/centroid map    \n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    \n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            cluster_index = word_centroid_map[word]\n",
    "            bag_of_centroids[cluster_index] += 1\n",
    "    \n",
    "    # Return the \"bag_of_centroids\"\n",
    "    return bag_of_centroids\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_bag_of_centroids` function returns **a numpy array for each review, each with a number of features equal to the number of clusters.** Finally, we create bags of centroids for our training and test set, then train a random forest and extract results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
